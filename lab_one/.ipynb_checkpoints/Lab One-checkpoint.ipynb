{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "----\n",
    "\n",
    "- Suppose we have v1, v2 web-page with 4 metrics \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Rates:\n",
      "Version v1: {'sign_up': 0.4, 'brochure_download': 0.3, 'try_play': 0.7}\n",
      "Version v2: {'sign_up': 0.34234234234234234, 'brochure_download': 0.43343343343343343, 'try_play': 0.6666666666666666}\n",
      "\n",
      "Statistical Significance (p-values) of the Difference between A and B:\n",
      "Sign-up: p-value = 0.0076156225572796025\n",
      "Brochure Download: p-value = 5.099801496940094e-10\n",
      "Try Play: p-value = 0.10928134790991598\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the summary table with conversion data\n",
    "summary_table = {\n",
    "    'version': ['v1', 'v2'],\n",
    "    'total_visitors': [1000, 999],\n",
    "    'sign_up_number': [400, 342],\n",
    "    'download_brochure_number': [300, 433],\n",
    "    'click_try_play_number': [700, 666]\n",
    "}\n",
    "\n",
    "# Calculate conversion rates for each version\n",
    "conversion_rates = {}\n",
    "outcomes = []\n",
    "for version in summary_table['version']:\n",
    "    \n",
    "    ## get the data\n",
    "    total_visitors = summary_table['total_visitors'][summary_table['version'].index(version)]\n",
    "    sign_up_number = summary_table['sign_up_number'][summary_table['version'].index(version)]\n",
    "    download_brochure_number = summary_table['download_brochure_number'][summary_table['version'].index(version)]\n",
    "    click_try_play_number = summary_table['click_try_play_number'][summary_table['version'].index(version)]\n",
    "    \n",
    "    \n",
    "    # Convert numbers to binary outcomes (1 for conversion, 0 for no conversion)\n",
    "    outcomes.append([1] * sign_up_number + [0] * (total_visitors - sign_up_number))\n",
    "    outcomes.append([1] * download_brochure_number + [0] * (total_visitors - download_brochure_number))\n",
    "    outcomes.append([1] * click_try_play_number + [0] * (total_visitors - click_try_play_number))\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate conversion rates for each metric separately\n",
    "    conversion_rate_sign_up = np.mean(outcomes[0 if version == 'v1' else 3])\n",
    "    conversion_rate_brochure = np.mean(outcomes[1 if version == 'v1' else 4])\n",
    "    conversion_rate_try_play = np.mean(outcomes[2 if version == 'v1' else 5])\n",
    "    \n",
    "    conversion_rates[version] = {\n",
    "        'sign_up': conversion_rate_sign_up,\n",
    "        'brochure_download': conversion_rate_brochure,\n",
    "        'try_play': conversion_rate_try_play\n",
    "    }\n",
    "\n",
    "# Perform hypothesis test (e.g., two-sample t-test) to determine if the difference in conversion rates is statistically significant\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Perform hypothesis tests for each conversion event between versions 1 and 2\n",
    "sign_up_t_statistic, sign_up_p_value = ttest_ind(outcomes[0], outcomes[3])\n",
    "brochure_t_statistic, brochure_p_value = ttest_ind(outcomes[1], outcomes[4])\n",
    "try_play_t_statistic, try_play_p_value = ttest_ind(outcomes[2], outcomes[5])\n",
    "\n",
    "# Print results\n",
    "print(\"Conversion Rates:\")\n",
    "for version, conversion_rate in conversion_rates.items():\n",
    "    print(f\"Version {version}: {conversion_rate}\")\n",
    "\n",
    "print(\"\\nStatistical Significance (p-values) of the Difference between A and B:\")\n",
    "print(f\"Sign-up: p-value = {sign_up_p_value}\")\n",
    "print(f\"Brochure Download: p-value = {brochure_p_value}\")\n",
    "print(f\"Try Play: p-value = {try_play_p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "- Sign-up and Brochure metrics has small p-value , which support the sginificant difference between V1 and V2\n",
    "- Try Play has large p-value and not sugest take into the score testing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sign_up': 0.4, 'brochure_download': 0.3, 'try_play': 0.7}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_rates[\"v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sign_up': 0.34234234234234234,\n",
       " 'brochure_download': 0.43343343343343343,\n",
       " 'try_play': 0.6666666666666666}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_rates[\"v2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page B is better.\n"
     ]
    }
   ],
   "source": [
    "## Scores\n",
    "\n",
    "# Assume you have already calculated the normalized metrics for both pages (A and B)\n",
    "normalized_metrics_A = conversion_rates[\"v1\"]\n",
    "normalized_metrics_B = conversion_rates[\"v2\"]\n",
    "\n",
    "# Define the weights for each metric\n",
    "weights = {'sign_up': 0.6, 'brochure_download': 0.35 ,'try_play': 0.05}\n",
    "\n",
    "# Calculate the composite scores for both pages\n",
    "composite_score_A = sum(normalized_metrics_A[metric] * weights[metric] for metric in normalized_metrics_A)\n",
    "composite_score_B = sum(normalized_metrics_B[metric] * weights[metric] for metric in normalized_metrics_B)\n",
    "\n",
    "# Compare the scores\n",
    "threshold = 0.01  # Define the threshold value\n",
    "\n",
    "if abs(composite_score_A - composite_score_B) < threshold:\n",
    "    print(\"Both pages have similar performance.\")\n",
    "elif composite_score_A > composite_score_B:\n",
    "    print(\"Page A is better.\")\n",
    "else:\n",
    "    print(\"Page B is better.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
